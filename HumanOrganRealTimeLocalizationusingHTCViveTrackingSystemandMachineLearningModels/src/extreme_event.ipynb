{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from spot import bidSPOT\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_N_INIT = 300\n",
    "\n",
    "filenames=['data/nonrigid_rollall_processed.csv', 'data/nonrigid_pitch2_processed.csv',\n",
    "'data/nonrigid_x4_processed.csv', 'data/nonrigid_y2_processed.csv',  'data/nonrigid_yaw2_processed.csv',\n",
    "'data/nonrigid_z_processed.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extreme_event_split(filename):\n",
    "\tdf = pd.read_csv(filename, sep=',')\n",
    "\tprint(df.size)\n",
    "\t# df = pd.read_csv('data/nonrigid_rollall_processed.csv', sep=',')\n",
    "\tdf = df.dropna()\n",
    "\t# print(df)\n",
    "\n",
    "\tX = df.iloc[:, 0]\n",
    "\t# X = df.dropna()\n",
    "\tX = pd.to_numeric(X, errors='coerce')\n",
    "\tX = X.values\n",
    "\n",
    "\tn_init = global_N_INIT\n",
    "\tinit_data = X[:n_init] \t# initial batch\n",
    "\tdata = X[n_init:]  \t\t# stream\n",
    "\n",
    "\tq = 1e-3\t\t\t\t# risk parameter\n",
    "\td = 100  \t\t\t\t# depth parameter\n",
    "\ts = bidSPOT(q,d)     \t# biDSPOT object\n",
    "\ts.fit(init_data,data) \t# data import\n",
    "\ts.initialize() \t  \t\t# initialization step\n",
    "\tresults = s.run()    \t# run\n",
    "\ts.plot(results)\n",
    "\n",
    "\tq_name = str(q)+'_depth'+str(d)\n",
    "\n",
    "\t# output_dir = \"C:\\\\Users\\\\12525\\\\Documents\\\\Thesis\\\\code\\\\test\\\\figures\\\\extreme_event\"\n",
    "\tname = Path(filename).stem\n",
    "\t# name = name + '_' + str(d)\n",
    "\tname = name + '_' + q_name+'.png'\n",
    "\n",
    "\tplt.title(str(q))\n",
    "\n",
    "\tplt.savefig('C:/Users/12525/Documents/Thesis/code/test/figures/extreme_event/{}'.format(name))\n",
    "\n",
    "\tplt.clf()\n",
    "\n",
    "\trun_ee_split(filename)\n",
    "\n",
    "def run_ee_split(filename):\n",
    "\n",
    "\t# df = pd.read_csv(filename, sep=',')\n",
    "\tdf = pd.read_csv(filename, sep=',')\n",
    "\n",
    "\n",
    "\t# print(df.isnull().any().any())\n",
    "\t# print(df.isnull().any())\n",
    "\t# df = df.astype(float)\n",
    "\tdf = df.dropna()\n",
    "\n",
    "\n",
    "\t# print(df.isnull().any().any())\n",
    "\t# print(df.isnull().any())\n",
    "\n",
    "\td = np.isfinite(df).any()\n",
    "\t# print(d)\n",
    "\n",
    "\tvalues = df.values\n",
    "\t# values = values.astype('float32')\n",
    "\n",
    "\tscaler = MinMaxScaler(feature_range=(0,1))\n",
    "\tscaled = scaler.fit_transform(values)\n",
    "\n",
    "\treframed = DataFrame(scaled)\n",
    "\tX = reframed.iloc[:, 0].dropna()\n",
    "\tX = pd.to_numeric(X, errors='coerce')\n",
    "\tX = X.values\n",
    "\n",
    "\t# X = reframed.iloc[:, 0].dropna(inplace=True)\n",
    "\t# X = reframed.iloc[:, 0].to_numpy(dtype='float32')\n",
    "\n",
    "# # reframed = DataFrame(scaled)\n",
    "\n",
    "\t# X = pd.to_numeric(X, errors='coerce')\n",
    "\t# X = X.values\n",
    "\n",
    "\tCV = 5\n",
    "\t# defines which chunk of the CV to visualize\n",
    "\t# e.g. CV=5 and VISUALIZE_CHUNK, this means visualize the 2nd of 5 chunks\n",
    "\tVISUALIZE_CHUNK = 5\n",
    "\n",
    "\tN_INIT = global_N_INIT\n",
    "\n",
    "\t# for threshold\n",
    "\t# for q_cms\n",
    "\t# results are totally different\n",
    "\t# q = 1e-5\n",
    "\tq = 1e-3\t\t\t\t# risk parameter, probability to have extreme events\n",
    "\t\t\t\t\t\t\t# like percentile to be the threshold, the higher the number\n",
    "\t\t\t\t\t\t\t# the less alarms (outliers)\n",
    "\td = 100  \t\t\t\t# depth parameter\n",
    "\n",
    "\t# for Turb\n",
    "\t# q = 0.03\n",
    "\n",
    "\t# # for SRP_mgPL\n",
    "\t# q = 0.1\n",
    "\t# d = 300\n",
    "\n",
    "\n",
    "\t# KERNEL = DecisionTreeRegressor(max_features='sqrt')\n",
    "\t# KERNEL = KNeighborsRegressor(n_neighbors=3)\n",
    "\t# KERNEL = MLPRegressor(learning_rate='adaptive', max_iter=500)\n",
    "\tKERNEL_EXTREME = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100,\n",
    "\t       subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1,\n",
    "\t       min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "\t       init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, \n",
    "\t       warm_start=False, validation_fraction=0.1, tol=0.0001)\n",
    "\n",
    "\tKERNEL_NORMAL = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100,\n",
    "\t       subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1,\n",
    "\t       min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "\t       init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, \n",
    "\t       warm_start=False, validation_fraction=0.1, tol=0.0001)\n",
    "\n",
    "\n",
    "\tTESTING_PERCENTAGE = 1/float(CV)\n",
    "\n",
    "\tX_backup = X.copy()\n",
    "\n",
    "\t# find threshold\n",
    "\tinit_data = X[:N_INIT] \t# initial batch\n",
    "\tdata = X[N_INIT:]  \t\t# stream\n",
    "\n",
    "\n",
    "\ts = bidSPOT(q,d)     \t# biDSPOT object\n",
    "\t# s = SPOT(q)\t\t     \t# biDSPOT object\n",
    "\ts.fit(init_data,data) \t# data import\n",
    "\ts.initialize() \t  \t\t# initialization step\n",
    "\tresults = s.run()    \t# run\n",
    "\n",
    "\ts.plot(results) \t \t# plot\n",
    "\n",
    "\toriginal = df.iloc[N_INIT:,:].copy()\n",
    "\t# restart index\n",
    "\toriginal = original.reset_index(drop='True')\n",
    "\n",
    "\ttotal_len = original.shape[0]\n",
    "\t# index of peaks results['alarms']\n",
    "\t# 20% for testing\n",
    "\t# 80% for training\n",
    "\ttest_data_len = int(total_len*TESTING_PERCENTAGE)\n",
    "\ttrain_data_len = total_len - test_data_len\n",
    "\n",
    "\n",
    "\tr_2_list = []\n",
    "\trmse_list = []\n",
    "\tvis_prediction = []\n",
    "\tvis_gap = []\n",
    "\tnormal_event_id = np.setdiff1d(range(total_len),results['alarms'])\n",
    "\tfor i in range(CV):\n",
    "\t\tdf_backup = df.iloc[N_INIT:,:].copy()\n",
    "\t\tdf_backup =df_backup.reset_index(drop='True')\n",
    "\t\t# decide where to make holes\n",
    "\t\tcount = 0\n",
    "\t\tprint(\"currently dealing with loop:\", i)\n",
    "\n",
    "\t\ttesting_start = test_data_len * i\n",
    "\t\ttesting_end = testing_start + test_data_len\n",
    "\t\ttesting_data_df = df_backup.iloc[testing_start:testing_end]\n",
    "\n",
    "\t\tif i == 0:\n",
    "\t\t\ttraining_data = df_backup.iloc[testing_end:]\n",
    "\t\telif i<=4:\n",
    "\t\t\ttraining_data_first = df_backup.iloc[:testing_start]\n",
    "\t\t\ttraining_data_second = df_backup.iloc[testing_end:]\n",
    "\t\t\ttraining_data = pd.concat([training_data_first,training_data_second])\n",
    "\t\telse:\n",
    "\t\t\ttraining_data = df_backup.iloc[:testing_start]\n",
    "\n",
    "\n",
    "\t\tflag_array_extreme = np.array([m for m in results['alarms'] if m<testing_start or m>testing_end])\n",
    "\t\t# flag_array_extreme = flag_array_extreme - global_N_INIT\n",
    "\t\t# extreme_training = training_data.reindex(columns=flag_array_extreme)  \n",
    "\t\textreme_training = training_data.loc[flag_array_extreme]\n",
    "\t\t# extreme_training = training_data.reindex(flag_array_extreme)\n",
    "\t\tflag_array_normal = np.array([m for m in normal_event_id if m<testing_start or m>testing_end])  \n",
    "\t\t# normal_training = training_data.reindex(columns=flag_array_normal)\n",
    "\t\t# normal_training = training_data[flag_array_normal].reindex(columns = flag_array_extreme)\n",
    "\t\tnormal_training = training_data.reindex(flag_array_normal)\n",
    "\n",
    "\t\t# print(\"normal: \", normal_training.shape)\n",
    "\t\t# normal_training = training_data.loc[flag_array_normal]\n",
    "\n",
    "\t# \textreme_training.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\t# \tnormal_training.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "\t# \textreme_training.fillna(flag_array_extreme.mean())\n",
    "\t# \tnormal_training.fillna(flag_array_normal.mean())\n",
    "\t\tprint(\"flag: \", flag_array_extreme.shape)\n",
    "\t\tprint(\"extreme: \", extreme_training)\n",
    "\t\tprint(\"max: \", np.max(flag_array_extreme))\n",
    "\t\tKERNEL_EXTREME.fit(extreme_training.iloc[:,1:].values,extreme_training.iloc[:,0].values) \n",
    "\t\tKERNEL_NORMAL.fit(normal_training.iloc[:,1:].values,normal_training.iloc[:,0].values)\n",
    "\n",
    "\t\t# testing\n",
    "\t\tx_testing_data = testing_data_df.iloc[:,1:].values\n",
    "\t\ty_testing_data = testing_data_df.iloc[:,0].values\n",
    "\t\tpredictions = []\n",
    "\t\tfor count in range(len(y_testing_data)):\n",
    "\t\t\trecord_index = testing_start+count\n",
    "\t\t\tif record_index in results['alarms']:\n",
    "\t\t\t\t# extreme event\n",
    "\t\t\t\ttmp_pred = KERNEL_EXTREME.predict([x_testing_data[count]])\n",
    "\t\t\telse:\n",
    "\t\t\t\t# normal event\n",
    "\t\t\t\ttmp_pred = KERNEL_NORMAL.predict([x_testing_data[count]])\n",
    "\t\t\tpredictions.append(tmp_pred)\n",
    "\n",
    "\t\ttmp_r_2 = r2_score(y_testing_data, predictions)\n",
    "\t\tr_2_list.append(tmp_r_2)\n",
    "\n",
    "\t\ttmp_rmse = mean_squared_error(y_testing_data, predictions, squared=False)\n",
    "\t\trmse_list.append(tmp_rmse)\n",
    "\t\tprint(\"length of rmse: \", len(rmse_list))\n",
    "\n",
    "\t# print(\"rmse list: \", rmse_list)\n",
    "\tprint(r_2_list)\n",
    "\tprint(\"avg_r2 is: \", sum(r_2_list) / len(r_2_list))\n",
    "\n",
    "\tprint(\"RMSE of Testing Extreme Event Split is displayed below:\")\n",
    "\t# print(np.average(mean_squared_error(test_y, reg_gbr.predict(test_x), squared=False)))\n",
    "\tprint(\"avg_rmse is: \", sum(rmse_list) / len(rmse_list))\n",
    "\n",
    "\n",
    "# for file in filenames:\n",
    "# \textreme_event_split(file)\n",
    "\n",
    "filenames=['data/nonrigid_rollall_processed.csv', 'data/nonrigid_pitch2_processed.csv',\n",
    "'data/nonrigid_x4_processed.csv', 'data/nonrigid_y2_processed.csv',  'data/nonrigid_yaw2_processed.csv',\n",
    "'data/nonrigid_z_processed.csv']\n",
    "\n",
    "tri_filenames=['data_triangleconfig/pitch7_triangle_processed.csv', 'data_triangleconfig/x_triangle_processed.csv', 'data_triangleconfig/y_triangle_processed.csv',\n",
    "'data_triangleconfig/z_triangle_processed.csv', 'data_triangleconfig/rollfinal_triangle_processed.csv',\n",
    "'data_triangleconfig/yaw_triangle_processed.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
